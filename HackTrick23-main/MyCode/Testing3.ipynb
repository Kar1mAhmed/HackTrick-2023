{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gym\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *\n",
    "\n",
    "import torch\n",
    "import pygame\n",
    "import time\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_obv(obv):\n",
    "    flattened = []\n",
    "    for dim in obv:\n",
    "        if isinstance(dim, list):\n",
    "            for arr in dim:\n",
    "                if isinstance(arr, list):\n",
    "                    for item in arr:\n",
    "                        flattened.append(item)\n",
    "                else:\n",
    "                    flattened.append(arr)\n",
    "        else:\n",
    "            flattened.append(dim)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        policy = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = 14\n",
    "a_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_hyper = {\n",
    "    \"h_size1\": 16,\n",
    "    \"h_size2\": 32,\n",
    "    \"h_size3\" : 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 5000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    state0 = state[0]\n",
    "    state1 = state[1]\n",
    "    state2 = state[2]\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    for i in state0:\n",
    "        ans.append(i)\n",
    "    \n",
    "    for i in state1:\n",
    "        ans.append(i)\n",
    "    \n",
    "    for i in state2:\n",
    "        for j in i:\n",
    "            ans.append(j)\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "agent_id = \"9\" # add your agent id here\n",
    "    \n",
    "manager = MazeManager()\n",
    "manager.init_maze(agent_id, maze_cells = sample_maze)\n",
    "env = manager.maze_map[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = s_size\n",
    "output_dim = a_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ActorCritic(input_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionss = ['N', 'S', 'E', 'W']\n",
    "def a2c(env, num_episodes, gamma, lr):\n",
    "    for i in range(num_episodes):\n",
    "        moves_count = {'W': 0, 'S' : 0, 'N': 0, 'E' :0}\n",
    "        obs = manager.reset(agent_id)\n",
    "        done = False\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        rid_recused = []\n",
    "        obs = convert_state(obs)\n",
    "        obs = np.array(obs)\n",
    "        steps_count = 0\n",
    "        while not done and steps_count < 5001:\n",
    "            manager.render(agent_id)\n",
    "            time.sleep(.1)\n",
    "            steps_count+= 1\n",
    "            states.append(obs)\n",
    "            policy, value = model(torch.tensor(obs).float().to(device))\n",
    "            action_probs = policy.detach().cpu().numpy()\n",
    "            action = np.random.choice(output_dim, p=action_probs)\n",
    "            pick_action = actionss[action]\n",
    "            moves_count[pick_action]+=1\n",
    "            obs, reward, done, _, info = manager.step(agent_id,pick_action)\n",
    "            obs = convert_state(obs)\n",
    "            obs = np.array(obs)\n",
    "            actions.append(action)\n",
    "            reward = 0\n",
    "            if info['riddle_type'] not in rid_recused:\n",
    "                rid_recused.append(info['riddle_type'])\n",
    "                reward = 1000\n",
    "            rewards.append(reward)\n",
    "        reward_sum = sum(rewards)\n",
    "        print(f\"\\nEpisode num : {i}, reward is : {reward_sum}, and moves count is {moves_count}\")\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).float().to(device)\n",
    "        states = torch.tensor(states).float().to(device)\n",
    "        actions = torch.tensor(actions).long().to(device)\n",
    "        policy, value = model(states)\n",
    "        advantage = returns - value.detach().squeeze()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        actor_loss = -torch.log(policy.gather(1, actions.unsqueeze(1)).squeeze() + 1e-10) * advantage.detach()\n",
    "        actor_loss = actor_loss.mean()\n",
    "        loss = actor_loss + critic_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode num : 0, reward is : 5000, and moves count is {'W': 1093, 'S': 1288, 'N': 1434, 'E': 1186}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a2c(env, \u001b[39m100\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m.001\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36ma2c\u001b[1;34m(env, num_episodes, batch_size, gamma, lr)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39mand\u001b[39;00m steps_count \u001b[39m<\u001b[39m \u001b[39m5001\u001b[39m:\n\u001b[0;32m     15\u001b[0m     manager\u001b[39m.\u001b[39mrender(agent_id)\n\u001b[1;32m---> 16\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m.1\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m     steps_count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     18\u001b[0m     states\u001b[39m.\u001b[39mappend(obs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a2c(env, 100, 256, 1, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rescue items\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m policy, action_probs \u001b[39m=\u001b[39m model(state)\n\u001b[0;32m     37\u001b[0m policy \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 38\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(output_dim, p\u001b[39m=\u001b[39;49maction_probs)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m pick_action \u001b[39m=\u001b[39m actionss[action]\n\u001b[0;32m     40\u001b[0m moves_count[pick_action]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mmtrand.pyx:944\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:958\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "actions = ['N', 'S', 'E', 'W']\n",
    "\n",
    "sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "agent_id = \"9\" # add your agent id here\n",
    "    \n",
    "manager = MazeManager()\n",
    "manager.init_maze(agent_id, maze_cells = sample_maze)\n",
    "env = manager.maze_map[agent_id]\n",
    "\n",
    "riddle_solvers = {'cipher': cipher_solver, 'captcha': captcha_solver, 'pcap': pcap_solver, 'server': server_solver}\n",
    "maze = {}\n",
    "states = {}\n",
    "\n",
    "    \n",
    "maze['maze'] = env.maze_view.maze.maze_cells.tolist()\n",
    "maze['rescue_items'] = list(manager.rescue_items_dict.keys())\n",
    "\n",
    "MAX_T = 5000\n",
    "RENDER_MAZE = True\n",
    "    \n",
    "\n",
    "with open(\"./states.json\", \"w\") as file:\n",
    "    json.dump(states, file)\n",
    "\n",
    "    \n",
    "with open(\"./maze.json\", \"w\") as file:\n",
    "    json.dump(maze, file)\n",
    "    \n",
    "state = manager.reset(agent_id)\n",
    "moves_count = {'W': 0, 'S' : 0, 'N': 0, 'E' :0}\n",
    "\n",
    "for i in range(5000):\n",
    "    state = convert_state(state)\n",
    "    state = np.array(state)\n",
    "    state = torch.tensor(state).float().to(device)\n",
    "    policy, action_probs = model(state)\n",
    "    policy = policy.detach().cpu().numpy()\n",
    "    \n",
    "    pick_action = actionss[action]\n",
    "    moves_count[pick_action]+=1\n",
    "    state, reward, done, truncated, info = manager.step(agent_id, pick_action)\n",
    "    if RENDER_MAZE:\n",
    "        manager.render(agent_id)\n",
    "print(moves_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
