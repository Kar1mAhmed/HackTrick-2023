{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gym\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *\n",
    "\n",
    "import pygame\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.97, num_actions=4, env_size=100):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = num_actions\n",
    "        self.env_size = env_size\n",
    "    \n",
    "    def remember(self, episode):\n",
    "        # episode = [env_state, action, reward, next_env_state, game_over]\n",
    "        # memory[i] = episode\n",
    "        # env_state == flattened 1d maze cells info, including agent cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def predict(self, env_state):\n",
    "        return self.model.predict(env_state)[0]\n",
    "    \n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.env_size   # env_state 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
    "            inputs[i] = env_state\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep (quote by Eder Santana)\n",
    "            targets[i] = self.predict(env_state)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(next_env_state))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qtraining(object):\n",
    "    def __init__(self, model, env,agent_id='9', **opt):\n",
    "        self.current_reward = 0\n",
    "        self.model = model  # Neural Network Model\n",
    "        self.env = env  # Environment (Tour De Flags maze object)\n",
    "        self.n_epoch = opt.get('n_epoch', 1000)  # Number of epochs to run\n",
    "        self.max_memory = opt.get('max_memory', 4* 100)  # Max memory for experiences\n",
    "        self.data_size = opt.get('data_size', int(0.75* 100))  # Data samples from experience replay\n",
    "        self.agent_cells = opt.get('agent_cells', [(0,0)])  # Starting cells for the agent\n",
    "        self.weights_file = opt.get('weights_file', \"\")  # Keras model weights file\n",
    "        self.name = opt.get('name', 'model')  # Name for saving weights and json files\n",
    "        self.rid_solved = []\n",
    "        self.agent_id = agent_id\n",
    "        self.win_count = 0\n",
    "        # If you want to continue training from a previous model,\n",
    "        # just supply the h5 file name to weights_file option\n",
    "        if self.weights_file:\n",
    "            print(\"loading weights from file: %s\" % (self.weights_file,))\n",
    "            self.model.load_weights(self.weights_file)\n",
    "\n",
    "        if self.agent_cells == 'all':\n",
    "            self.agent_cells = self.env.free_cells\n",
    "\n",
    "        # Initialize experience replay object\n",
    "        self.experience = Experience(self.model, max_memory=self.max_memory)\n",
    "        self.Actions = ['N', 'S', 'E', 'W']\n",
    "\n",
    "    def train(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        self.seconds = 0\n",
    "        self.win_count = 0\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.epoch = epoch\n",
    "            self.loss = 0.0\n",
    "            self.env.reset(self.agent_id)\n",
    "            game_over = False\n",
    "            # get initial env_state (1d flattened canvas)\n",
    "            self.env_state = self.env.reset(self.agent_id)\n",
    "            self.n_episodes = 0\n",
    "            while not game_over:\n",
    "                game_over = self.play()\n",
    "\n",
    "            dt = datetime.datetime.now() - start_time\n",
    "            self.seconds = dt.total_seconds()\n",
    "            t = format_time(self.seconds)\n",
    "            fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | flags: {:d} | e: {:.3f} | time: {}\"\n",
    "            print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.flags), self.epsilon(), t))\n",
    "            if self.win_count > 2:\n",
    "                if self.completion_check():\n",
    "                    print(\"Completed training at epoch: %d\" % (epoch,))\n",
    "                    break\n",
    "                \n",
    "    def play(self):\n",
    "        action = self.action()\n",
    "        prev_env_state = self.env_state\n",
    "        self.env_state, reward, game_status, truncated, info = self.env.step(self.agent_id, action)\n",
    "        if info['riddle_type'] not in self.rid_solved:\n",
    "            reward+= 1000\n",
    "            self.rid_solved.append(info['riddle_type'])\n",
    "        game_status = self.env.is_game_over(self.agent_id)\n",
    "        if game_status == True:\n",
    "            self.win_count += 1\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        # Store episode (experience)\n",
    "        episode = [prev_env_state, action, reward, self.env_state, game_over]\n",
    "        self.experience.remember(episode)\n",
    "        self.n_episodes += 1\n",
    "\n",
    "        # Train model\n",
    "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
    "        epochs = int(self.env.base)\n",
    "        h = self.model.fit(\n",
    "            inputs,\n",
    "            targets,\n",
    "            epochs = epochs,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
    "        return game_over\n",
    "    \n",
    "    def action(self):\n",
    "        # Get next action\n",
    "        valid_actions = self.Actions\n",
    "        if not valid_actions:\n",
    "            action = None\n",
    "        elif np.random.rand() < self.epsilon():\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            q = self.experience.predict(self.env_state)\n",
    "            action = np.argmax(q)\n",
    "        return action\n",
    "    \n",
    "    def epsilon(self):\n",
    "        n = self.win_count\n",
    "        top = 0.80\n",
    "        bottom = 0.08\n",
    "        if n<10:\n",
    "            e = bottom + (top - bottom) / (1 + 0.1 * n**0.5)\n",
    "        else:\n",
    "            e = bottom\n",
    "        return e\n",
    "    \n",
    "    def completion_check(self):\n",
    "        for agent in self.agent_cells:\n",
    "            if not self.run_game(agent):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def save(self, name=\"\"):\n",
    "        # Save trained model weights and architecture, this will be used by the visualization code\n",
    "        if not name:\n",
    "            name = self.name\n",
    "        h5file = 'model_%s.h5' % (name,)\n",
    "        json_file = 'model_%s.json' % (name,)\n",
    "        self.model.save_weights(h5file, overwrite=True)\n",
    "        with open(json_file, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "        t = format_time(self.seconds)\n",
    "        print('files: %s, %s' % (h5file, json_file))\n",
    "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env, **opt):\n",
    "    loss = opt.get('loss', 'mse')\n",
    "    a = opt.get('alpha', 0.24)\n",
    "    model = Sequential()\n",
    "    esize = 10\n",
    "    model.add(Dense(esize, input_shape=(esize,)))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "agent_id = \"9\" # add your agent id here\n",
    "manager = MazeManager()\n",
    "manager.init_maze(agent_id, maze_cells=sample_maze)\n",
    "env = manager.maze_map[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0]), [13, 11, 3, 10], [[1, 1], [1, 1], [1, 0], [1, 1]]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.reset('9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model(manager)\n\u001b[0;32m      3\u001b[0m qt \u001b[39m=\u001b[39m Qtraining(\n\u001b[0;32m      4\u001b[0m     model,\n\u001b[0;32m      5\u001b[0m     manager,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel_1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m qt\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[43], line 42\u001b[0m, in \u001b[0;36mQtraining.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_episodes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m game_over:\n\u001b[1;32m---> 42\u001b[0m     game_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay()\n\u001b[0;32m     44\u001b[0m dt \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseconds \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mtotal_seconds()\n",
      "Cell \u001b[1;32mIn[43], line 71\u001b[0m, in \u001b[0;36mQtraining.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_episodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m inputs, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperience\u001b[39m.\u001b[39;49mget_data(data_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_size)\n\u001b[0;32m     72\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbase)\n\u001b[0;32m     73\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     74\u001b[0m     inputs,\n\u001b[0;32m     75\u001b[0m     targets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m     79\u001b[0m )\n",
      "Cell \u001b[1;32mIn[42], line 29\u001b[0m, in \u001b[0;36mExperience.get_data\u001b[1;34m(self, data_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(mem_size), data_size, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[0;32m     28\u001b[0m     env_state, action, reward, next_env_state, game_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory[j]\n\u001b[1;32m---> 29\u001b[0m     inputs[i] \u001b[39m=\u001b[39m env_state\n\u001b[0;32m     30\u001b[0m     \u001b[39m# There should be no target values for actions not taken.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Thou shalt not correct actions not taken #deep (quote by Eder Santana)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     targets[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(env_state)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "model = build_model(manager)\n",
    "\n",
    "qt = Qtraining(\n",
    "    model,\n",
    "    manager,\n",
    "    n_epoch = 200,\n",
    "    max_memory = 500,\n",
    "    data_size = 100,\n",
    "    name = 'model_1'\n",
    ")\n",
    "\n",
    "qt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
