{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gym\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *\n",
    "\n",
    "import pygame\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n",
    "# from stable_baselines3 import A2C\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    # This is a random agent \n",
    "    # This function should get actions from your trained agent when inferencing.\n",
    "    actions = ['N', 'S', 'E', 'W']\n",
    "    random_action = random.choice(actions)\n",
    "    action_index = actions.index(random_action)\n",
    "    return random_action, action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_inference(riddle_solvers):\n",
    "\n",
    "    obv = manager.reset(agent_id)\n",
    "\n",
    "    for t in range(MAX_T):\n",
    "        # Select an action\n",
    "        state_0 = obv\n",
    "        action, action_index = select_action(state_0) # Random action\n",
    "        obv, reward, terminated, truncated, info = manager.step(agent_id, action)\n",
    "        print(obv)\n",
    "        if not info['riddle_type'] == None:\n",
    "            solution = riddle_solvers[info['riddle_type']](info['riddle_question'])\n",
    "            obv, reward, terminated, truncated, info = manager.solve_riddle(info['riddle_type'], agent_id, solution)\n",
    "\n",
    "        # THIS IS A SAMPLE TERMINATING CONDITION WHEN THE AGENT REACHES THE EXIT\n",
    "        # IMPLEMENT YOUR OWN TERMINATING CONDITION\n",
    "        if np.array_equal(obv[0], (9,9)):\n",
    "            manager.set_done(agent_id)\n",
    "            break # Stop Agent\n",
    "\n",
    "        if RENDER_MAZE:\n",
    "            manager.render(agent_id)\n",
    "\n",
    "        states[t] = [obv[0].tolist(), action_index, str(manager.get_rescue_items_status(agent_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m MAX_T \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m     19\u001b[0m RENDER_MAZE \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m local_inference(riddle_solvers)\n\u001b[0;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./states.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     24\u001b[0m     json\u001b[39m.\u001b[39mdump(states, file)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mlocal_inference\u001b[1;34m(riddle_solvers)\u001b[0m\n\u001b[0;32m      7\u001b[0m state_0 \u001b[39m=\u001b[39m obv\n\u001b[0;32m      8\u001b[0m action, action_index \u001b[39m=\u001b[39m select_action(state_0) \u001b[39m# Random action\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m obv, reward, terminated, truncated, info \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39;49mstep(agent_id, action)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(obv)\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mriddle_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Coding\\DEll Winers\\HackTrick23\\gym-maze\\gym_maze\\envs\\maze_manager.py:104\u001b[0m, in \u001b[0;36mMazeManager.step\u001b[1;34m(self, agent_id, action)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, agent_id, action):\n\u001b[1;32m--> 104\u001b[0m     obv, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaze_map[agent_id]\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtuple\u001b[39m(obv[\u001b[39m0\u001b[39m]) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescue_items_dict:\n\u001b[0;32m    106\u001b[0m         riddle_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescue_items_dict[\u001b[39mtuple\u001b[39m(obv[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32me:\\Coding\\DEll Winers\\HackTrick23\\gym-maze\\gym_maze\\envs\\maze_env.py:86\u001b[0m, in \u001b[0;36mMazeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_view\u001b[39m.\u001b[39mmove_robot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mACTION[action])\n\u001b[0;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaze_view\u001b[39m.\u001b[39;49mmove_robot(action)\n\u001b[0;32m     88\u001b[0m distances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_view\u001b[39m.\u001b[39mget_rescue_items_locations()[\u001b[39m0\u001b[39m]\n\u001b[0;32m     89\u001b[0m directions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_view\u001b[39m.\u001b[39mget_rescue_items_locations()[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32me:\\Coding\\DEll Winers\\HackTrick23\\gym-maze\\gym_maze\\envs\\maze_view_2d.py:138\u001b[0m, in \u001b[0;36mMazeView2D.move_robot\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmove_robot\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mdir\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39mdir\u001b[39;49m)\n\u001b[0;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mdir\u001b[39m,\u001b[39mint\u001b[39m):\n\u001b[0;32m    141\u001b[0m         \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__maze\u001b[39m.\u001b[39mCOMPASS\u001b[39m.\u001b[39mkeys())[\u001b[39mdir\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "    agent_id = \"2\" # add your agent id here\n",
    "    \n",
    "    manager = MazeManager()\n",
    "    manager.init_maze(agent_id, maze_cells=sample_maze)\n",
    "    env = manager.maze_map[agent_id]\n",
    "\n",
    "    riddle_solvers = {'cipher': cipher_solver, 'captcha': captcha_solver, 'pcap': pcap_solver, 'server': server_solver}\n",
    "    maze = {}\n",
    "    states = {}\n",
    "\n",
    "    \n",
    "    maze['maze'] = env.maze_view.maze.maze_cells.tolist()\n",
    "    maze['rescue_items'] = list(manager.rescue_items_dict.keys())\n",
    "\n",
    "    MAX_T = 5000\n",
    "    RENDER_MAZE = True\n",
    "    \n",
    "    local_inference(riddle_solvers)\n",
    "\n",
    "    with open(\"./states.json\", \"w\") as file:\n",
    "        json.dump(states, file)\n",
    "\n",
    "    \n",
    "    with open(\"./maze.json\", \"w\") as file:\n",
    "        json.dump(maze, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (14,) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m A2C(\u001b[39m'\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m'\u001b[39;49m, env)\u001b[39m.\u001b[39;49mlearn(\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:193\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m: SelfA2C,\n\u001b[0;32m    185\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    191\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    194\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    195\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    196\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    197\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    198\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    199\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:236\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    226\u001b[0m     \u001b[39mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    227\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    234\u001b[0m     iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 236\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[0;32m    237\u001b[0m         total_timesteps,\n\u001b[0;32m    238\u001b[0m         callback,\n\u001b[0;32m    239\u001b[0m         reset_num_timesteps,\n\u001b[0;32m    240\u001b[0m         tb_log_name,\n\u001b[0;32m    241\u001b[0m         progress_bar,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[0;32m    244\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    246\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:408\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m reset_num_timesteps \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m    410\u001b[0m     \u001b[39m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:74\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 74\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     75\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:75\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m     74\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_obs(env_idx, obs)\n\u001b[0;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32mc:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:105\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[1;34m(self, env_idx, obs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[0;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[0;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (14,) into shape (2,)"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env).learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
