{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gym\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *\n",
    "\n",
    "import pygame\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.97):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = 4\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [env_state, action, reward, next_env_state, game_over]\n",
    "        # memory[i] = episode\n",
    "        # env_state == flattened 1d maze cells info, including agent cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, env_state):\n",
    "        return self.model.predict(env_state)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # env_state 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
    "            inputs[i] = env_state\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep (quote by Eder Santana)\n",
    "            targets[i] = self.predict(env_state)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(next_env_state))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qtraining(object):\n",
    "    def __init__(self, model, env, **opt):\n",
    "        self.model = model  # Nueral Network Model\n",
    "        self.env = env  # Environment (Tour De Flags maze object)\n",
    "        self.n_epoch = opt.get('n_epoch', 1000)  # Number of epochs to run\n",
    "        self.max_memory = opt.get('max_memory', 4*100)  # Max memory for experiences\n",
    "        self.data_size = opt.get('data_size', int(0.75*100))  # Data samples from experience replay\n",
    "        self.agent_cells = opt.get('agent_cells', [(0,0)])  # Starting cells for the agent\n",
    "        self.weights_file = opt.get('weights_file', \"\")  # Keras model weights file\n",
    "        self.name = opt.get('name', 'model')  # Name for saving weights and json files\n",
    "\n",
    "        self.win_count = 0\n",
    "        # If you want to continue training from a previous model,\n",
    "        # just supply the h5 file name to weights_file option\n",
    "        if self.weights_file:\n",
    "            print(\"loading weights from file: %s\" % (self.weights_file,))\n",
    "            self.model.load_weights(self.weights_file)\n",
    "\n",
    "        if self.agent_cells == 'all':\n",
    "            self.agent_cells = self.env.free_cells\n",
    "\n",
    "        # Initialize experience replay object\n",
    "        self.experience = Experience(self.model, max_memory=self.max_memory)\n",
    "        self.agent_id = '9'\n",
    "        \n",
    "        self.Actions = ['N', 'S', 'E', 'W']\n",
    "        self.rescued = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        self.seconds = 0\n",
    "        self.win_count = 0\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.epoch = epoch\n",
    "            self.loss = 0.0\n",
    "            agent = random.choice(self.agent_cells)\n",
    "            self.env.reset(self.agent_id)\n",
    "            game_over = False\n",
    "            # get initial env_state (1d flattened canvas)\n",
    "            self.env_state = self.env.get_observation_space(self.agent_id)\n",
    "            self.n_episodes = 0\n",
    "            while not game_over:\n",
    "                game_over = self.play()\n",
    "\n",
    "            dt = datetime.datetime.now() - start_time\n",
    "            self.seconds = dt.total_seconds()\n",
    "            t = format_time(self.seconds)\n",
    "            fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | flags: {:d} | e: {:.3f} | time: {}\"\n",
    "            print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.flags), self.epsilon(), t))\n",
    "            if self.win_count > 2:\n",
    "                if self.completion_check():\n",
    "                    print(\"Completed training at epoch: %d\" % (epoch,))\n",
    "                    break\n",
    "\n",
    "    def play(self):\n",
    "        action = self.action()\n",
    "        prev_env_state = self.env_state\n",
    "\n",
    "        self.env_state, reward, game_status, tranc, info = self.env.step(self.agent_id, action)\n",
    "        fixed_state = self.fix_state(self.env_state)\n",
    "        if info['rescued_items'] == 4 and fixed_state[0] == 9 and fixed_state[1] == 9:\n",
    "            self.win_count += 1\n",
    "            reward = 3\n",
    "            game_over = True\n",
    "        elif fixed_state[0] == 9 and fixed_state[1] == 9:\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        if prev_env_state[0][0] == fixed_state[0] and prev_env_state[0][1] == fixed_state[1]:\n",
    "            reward -= 0.4\n",
    "        elif info['riddle_type'] != None and info['riddle_type'] not in self.rescued:\n",
    "            reward += 1\n",
    "        else:\n",
    "            reward-= 0.1\n",
    "        # Store episode (experience)\n",
    "        \n",
    "        episode = [prev_env_state, action, reward, fixed_state, game_over]\n",
    "        self.experience.remember(episode)\n",
    "        self.n_episodes += 1\n",
    "\n",
    "        # Train model\n",
    "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
    "        epochs = int(self.env.base)\n",
    "        h = self.model.fit(\n",
    "            inputs,\n",
    "            targets,\n",
    "            epochs = epochs,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
    "        return game_over\n",
    "\n",
    "    def run_game(self, agent):\n",
    "        self.env.reset(self.agent_id)\n",
    "        env_state = self.env.get_observation_space(self.agent_id)\n",
    "        while True:\n",
    "            # get next action\n",
    "            env_state = self.fix_state(env_state)\n",
    "            q = self.model.predict(env_state)\n",
    "            action = np.argmax(q[0])\n",
    "            prev_env_state = env_state\n",
    "            # apply action, get rewards and new state\n",
    "            env_state, reward, game_status,tranc, info = self.env.step(action)\n",
    "            fixed_state = self.fix_state(env_state)\n",
    "            if info['rescued_items'] == 4 and fixed_state[0] == 9 and fixed_state[1] == 9:\n",
    "                return True\n",
    "            elif game_status == 'lose':\n",
    "                return False\n",
    "\n",
    "    def action(self):\n",
    "        # Get next action\n",
    "        valid_actions = self.Actions\n",
    "        if not valid_actions:\n",
    "            action = None\n",
    "        elif np.random.rand() < self.epsilon():\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            q = self.experience.predict(self.env_state)\n",
    "            action = np.argmax(q)\n",
    "        return action\n",
    "\n",
    "    def epsilon(self):\n",
    "        n = self.win_count\n",
    "        top = 0.80\n",
    "        bottom = 0.08\n",
    "        if n<10:\n",
    "            e = bottom + (top - bottom) / (1 + 0.1 * n**0.5)\n",
    "        else:\n",
    "            e = bottom\n",
    "        return e\n",
    "    \n",
    "    def completion_check(self):\n",
    "        for agent in self.agent_cells:\n",
    "            if not self.run_game(agent):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def save(self, name=\"\"):\n",
    "        # Save trained model weights and architecture, this will be used by the visualization code\n",
    "        if not name:\n",
    "            name = self.name\n",
    "        h5file = 'model_%s.h5' % (name,)\n",
    "        json_file = 'model_%s.json' % (name,)\n",
    "        self.model.save_weights(h5file, overwrite=True)\n",
    "        with open(json_file, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "        t = format_time(self.seconds)\n",
    "        print('files: %s, %s' % (h5file, json_file))\n",
    "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))\n",
    "        \n",
    "        \n",
    "    def fix_state(self, state):\n",
    "        state0 = state[0]\n",
    "        state1 = state[1]\n",
    "        state2 = state[2]\n",
    "    \n",
    "        ans = []\n",
    "        for i in state0:\n",
    "            ans.append(i)\n",
    "        for i in state1:\n",
    "            ans.append(i)\n",
    "        for i in state2:\n",
    "            for j in i:\n",
    "                ans.append(j)\n",
    "        return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(**opt):\n",
    "    loss = opt.get('loss', 'mse')\n",
    "    a = opt.get('alpha', 0.24)\n",
    "    model = Sequential()\n",
    "    esize = 100\n",
    "    model.add(Dense(esize, input_shape=(esize,)))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "agent_id = \"9\" # add your agent id here\n",
    "manager = MazeManager()\n",
    "manager.init_maze(agent_id, maze_cells=sample_maze)\n",
    "env = manager.maze_map[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model()\n\u001b[0;32m      3\u001b[0m qt \u001b[39m=\u001b[39m Qtraining(\n\u001b[0;32m      4\u001b[0m     model,\n\u001b[0;32m      5\u001b[0m     manager,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel_1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m qt\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mQtraining.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_episodes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m game_over:\n\u001b[1;32m---> 45\u001b[0m     game_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay()\n\u001b[0;32m     47\u001b[0m dt \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseconds \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mtotal_seconds()\n",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m, in \u001b[0;36mQtraining.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction()\n\u001b[0;32m     59\u001b[0m prev_env_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_state\n\u001b[1;32m---> 61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_state, reward, game_status, tranc, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent_id, action)\n\u001b[0;32m     62\u001b[0m fixed_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfix_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_state)\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mrescued_items\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m fixed_state[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m \u001b[39mand\u001b[39;00m fixed_state[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m:\n",
      "File \u001b[1;32me:\\Dell Winers\\HackTrick23-main\\gym-maze\\gym_maze\\envs\\maze_manager.py:104\u001b[0m, in \u001b[0;36mMazeManager.step\u001b[1;34m(self, agent_id, action)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, agent_id, action):\n\u001b[1;32m--> 104\u001b[0m     obv, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaze_map[agent_id]\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtuple\u001b[39m(obv[\u001b[39m0\u001b[39m]) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescue_items_dict:\n\u001b[0;32m    106\u001b[0m         riddle_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescue_items_dict[\u001b[39mtuple\u001b[39m(obv[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "qt = Qtraining(\n",
    "    model,\n",
    "    manager,\n",
    "    n_epoch = 200,\n",
    "    max_memory = 500,\n",
    "    data_size = 100,\n",
    "    name = 'model_1'\n",
    ")\n",
    "\n",
    "qt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
