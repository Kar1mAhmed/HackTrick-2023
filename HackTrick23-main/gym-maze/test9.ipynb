{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gym\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from gym_maze.envs.maze_view_2d import MazeView2D\n",
    "\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\", \"rgb_array\"],\n",
    "    }\n",
    "\n",
    "    ACTION = [\"N\", \"S\", \"E\", \"W\"]\n",
    "\n",
    "    def __init__(self, maze_file=None, maze_cells=None, maze_size=None, mode=None, enable_render=True, rescue_item_locations=None, has_loops=False):\n",
    "        super().__init__()\n",
    "        self.viewer = None\n",
    "        self.enable_render = enable_render\n",
    "        self.rescue_item_locations= rescue_item_locations\n",
    "\n",
    "\n",
    "        if hasattr(maze_cells, 'shape'):\n",
    "            self.maze_view = MazeView2D(maze_name=\"OpenAI Gym - Maze (10 x 10)\",\n",
    "                                        maze_cells=maze_cells,\n",
    "                                        screen_size=(640, 640), \n",
    "                                        enable_render=enable_render, rescue_item_locations=self.rescue_item_locations)\n",
    "        \n",
    "        elif maze_file:\n",
    "            self.maze_view = MazeView2D(maze_name=\"OpenAI Gym - Maze (%s)\" % maze_file,\n",
    "                                        maze_file_path=maze_file,\n",
    "                                        screen_size=(640, 640), \n",
    "                                        enable_render=enable_render, rescue_item_locations=self.rescue_item_locations)\n",
    "        elif maze_size:\n",
    "            self.maze_view = MazeView2D(maze_name=\"OpenAI Gym - Maze (%d x %d)\" % maze_size,\n",
    "                                        maze_size=maze_size, screen_size=(640, 640),\n",
    "                                        has_loops=has_loops,\n",
    "                                        enable_render=enable_render, rescue_item_locations=self.rescue_item_locations)\n",
    "        else:\n",
    "            raise AttributeError(\"One must supply either a maze_file path (str) or the maze_size (tuple of length 2)\")\n",
    "\n",
    "        self.maze_size = self.maze_view.maze_size\n",
    "\n",
    "        # forward or backward in each dimension\n",
    "        self.action_space = spaces.Discrete(2*len(self.maze_size))\n",
    "        self.final_pos = (maze_size[0]-1,maze_size[1]-1)\n",
    "        # observation is the x, y coordinate of the grid\n",
    "        low = np.zeros(len(self.maze_size), dtype=int)\n",
    "        high =  np.array(self.maze_size, dtype=int) - np.ones(len(self.maze_size), dtype=int)\n",
    "        self.observation_space = spaces.Box(0, 150, dtype=np.uint8, shape = (114,))\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "        self.action_space = spaces.Discrete(4) \n",
    "        self.observation_space = spaces.Dict({\n",
    "            'my_feature': spaces.Box(low=0, high=120, shape=(114,), dtype=np.float32)\n",
    "        #     'desired_goal': spaces.Box(low=0, high=120, shape=(114,), dtype=np.float32),\n",
    "        #     'achieved_goal': spaces.Box(low=0, high=120, shape=(114,), dtype=np.float32)\n",
    "        })\n",
    "        # initial condition\n",
    "        self.state = None\n",
    "        self.steps_beyond_done = None\n",
    "        # Simulation related variables.\n",
    "        self.reset()\n",
    "\n",
    "        # Just need to initialize the relevant attributes\n",
    "        self.configure()\n",
    "        self.seed()\n",
    "\n",
    "        self.target_locations = self.maze_view.rescue_item_locations\n",
    "        self.target_reached = 0\n",
    "        self.tot_reward = 0\n",
    "\n",
    "        # self.maze_view.maze.save_maze('./hacktrick.npy')\n",
    "\n",
    "        self.rescued = 0\n",
    "        self.last_pos_x =self.maze_view.robot[0]\n",
    "        self.last_pos_y = self.maze_view.robot[1]\n",
    "        \n",
    "        self.cur_pos_x = 0\n",
    "        self.cur_pos_y = 0\n",
    "        \n",
    "        self.last_action = 0\n",
    "        self.steps = None\n",
    "        self.steps_count = 0\n",
    "        \n",
    "        self.temp = []\n",
    "        self.visited = []\n",
    "        \n",
    "        self.done = np.zeros((10,10))\n",
    "        self.done[0][0] = 1\n",
    "        self.state = self.add_done_to_state()\n",
    "\n",
    "        \n",
    "    def __del__(self):\n",
    "        if self.enable_render is True:\n",
    "            self.maze_view.quit_game()\n",
    "\n",
    "    def configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        info = {}\n",
    "        self.last_pos_x = self.maze_view.robot[0]\n",
    "        self.last_pos_y = self.maze_view.robot[1]\n",
    "        \n",
    "        \n",
    "        if isinstance(action, int):\n",
    "            self.maze_view.move_robot(self.ACTION[action])\n",
    "        else:\n",
    "            self.maze_view.move_robot(self.ACTION[action])\n",
    "\n",
    "        distances = self.maze_view.get_rescue_items_locations()[0]\n",
    "        directions = self.maze_view.get_rescue_items_locations()[1]\n",
    "\n",
    "        info['rescued_items'] = self.maze_view.rescued_items\n",
    "\n",
    "\n",
    "        self.state = [self.maze_view.robot, distances, directions]\n",
    "        # self.state = self.maze_view.robot\n",
    "        # print(\"new state\",self.state_new)\n",
    "        self.state = self.fix_state()\n",
    "        \n",
    "        np.append(self.state, self.last_action)\n",
    "        self.last_action = action\n",
    "        \n",
    "        \n",
    "        self.cur_pos_x = self.state[0]\n",
    "        self.cur_pos_y = self.state[1]\n",
    "        \n",
    "        self.done[self.maze_view.robot[0]][self.maze_view.robot[0]] = 1\n",
    "        \n",
    "        truncated = False\n",
    "        \n",
    "        if self.steps_count > 5000:\n",
    "            truncated = True\n",
    "            \n",
    "        info['riddle_type'] = None\n",
    "        info['riddle_question'] = None\n",
    "        \n",
    "        reward = self.compute_reward()\n",
    "        terminated = False\n",
    "        self.tot_reward += reward\n",
    "\n",
    "        if self.tot_reward < -500:\n",
    "            terminated = True\n",
    "            \n",
    "        self.steps +=1\n",
    "        self.steps_count+=1\n",
    "        self.normalize_state()\n",
    "        \n",
    "        self.temp = []\n",
    "        self.temp.append(self.state[0])\n",
    "        self.temp.append(self.state[1])\n",
    "        \n",
    "        self.state = self.add_done_to_state()\n",
    "        return self.state, reward, terminated, info\n",
    "\n",
    "    def add_done_to_state(self):\n",
    "        goal = []\n",
    "        for i in self.state:\n",
    "            goal.append(i)\n",
    "            \n",
    "        for arr in self.done:\n",
    "            for n in arr:\n",
    "                goal.append(n)\n",
    "        \n",
    "        return np.array(goal, dtype=float)\n",
    "                \n",
    "    def compute_reward(self):\n",
    "        \n",
    "        if self.temp not in self.visited:\n",
    "            self.visited.append(self.temp)\n",
    "            return 0.2\n",
    "        reached = False\n",
    "        \n",
    "        \n",
    "        new_targets = []\n",
    "        for i in self.target_locations:\n",
    "            if (self.cur_pos_x == i[0]) and (self.cur_pos_y == i[1]):\n",
    "                reached = True\n",
    "                continue\n",
    "            new_targets.append(i)\n",
    "        self.target_locations = new_targets\n",
    "        \n",
    "        if reached:\n",
    "            return 0.3\n",
    "    \n",
    "        elif (self.state[0]  == self.last_pos_x) and (self.last_pos_y == self.state[1]):\n",
    "            return -0.1\n",
    "        \n",
    "        elif (self.state[0] == 9) and (self.state[1] == 9) :\n",
    "            self.terminated = True\n",
    "            return 0.5 * (4 - len(self.target_locations))\n",
    "        \n",
    "        else:\n",
    "            return -0.01\n",
    "        \n",
    "        \n",
    "    def get_current_state(self):\n",
    "        info = {}\n",
    "        distances = self.maze_view.get_rescue_items_locations()[0]\n",
    "        directions = self.maze_view.get_rescue_items_locations()[1]\n",
    "\n",
    "        info['rescued_items'] = self.maze_view.rescued_items\n",
    "\n",
    "\n",
    "        self.state = [self.maze_view.robot, distances, directions]\n",
    "        truncated = False\n",
    "        \n",
    "        info['riddle_type'] = None\n",
    "        info['riddle_question'] = None\n",
    "        \n",
    "        reward = None\n",
    "        terminated = False\n",
    "\n",
    "        return self.state, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = np.zeros((10,10))\n",
    "        self.done[0][0] = 1\n",
    "        self.rescue_item_locations = self.get_rescue_points()\n",
    "        self.seed()\n",
    "        self.target_locations = self.maze_view.rescue_item_locations\n",
    "        self.target_reached = 0\n",
    "        self.tot_reward = 0\n",
    "        # self.maze_view.maze.save_maze('./hacktrick.npy')\n",
    "        self.rescued = 0\n",
    "        self.last_pos_x =self.maze_view.robot[0]\n",
    "        self.last_pos_y = self.maze_view.robot[1]\n",
    "        self.cur_pos_x = 0\n",
    "        self.cur_pos_y = 0\n",
    "        self.last_action = 0\n",
    "        self.steps = None\n",
    "        self.steps_count = 0\n",
    "        self.visited = []\n",
    "        self.maze_view.reset_robot()\n",
    "        self.state = [self.maze_view.robot, self.maze_view.get_rescue_items_locations()[0], self.maze_view.get_rescue_items_locations()[1]]\n",
    "        self.steps_beyond_done = None\n",
    "        self.steps = 0\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "        self.maze_view.reset_rescue_items()\n",
    "        self.state = self.fix_state()\n",
    "        self.tot_reward = 0\n",
    "        self.temp = []\n",
    "        self.visited = []\n",
    "        self.last_pos_x = 0\n",
    "        self.last_pos_y = 0\n",
    "        self.last_action =0\n",
    "        np.append(self.state, self.last_action)\n",
    "        self.pos = (0,0)\n",
    "        self.target_locations = self.maze_view.rescue_item_locations\n",
    "        self.normalize_state()\n",
    "        self.state = self.add_done_to_state()\n",
    "        return self.state\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.maze_view.game_over\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        if close:\n",
    "            self.maze_view.quit_game()\n",
    "\n",
    "        return self.maze_view.update(mode)\n",
    "\n",
    "    def fix_state(self):\n",
    "        state = self.state\n",
    "        state0 = state[0]\n",
    "        state1 = state[1]\n",
    "        state2 = state[2]\n",
    "    \n",
    "        ans = []\n",
    "        for i in state0:\n",
    "            ans.append(i)\n",
    "        for i in state1:\n",
    "            ans.append(i)\n",
    "        for i in state2:\n",
    "            for j in i:\n",
    "                ans.append(j)\n",
    "        return np.array(ans, dtype=float)\n",
    "    \n",
    "    def normalize_state(self):\n",
    "        dis_sum = sum(self.state[2:6]) \n",
    "        self.state[2] /= dis_sum\n",
    "        self.state[3] /= dis_sum\n",
    "        self.state[4] /= dis_sum\n",
    "        self.state[5] /= dis_sum\n",
    "        \n",
    "        self.state[0] /= 9\n",
    "        self.state[1] /= 9\n",
    "    \n",
    "    def get_rescue_points(self):\n",
    "        random_tuples = set()  # Create an empty set to hold the tuples \n",
    "\n",
    "        while len(random_tuples) < 4:  # Continue until we have 4 unique tuples \n",
    "            x = random.randint(1, 8) \n",
    "            y = random.randint(1, 8) \n",
    "            new_tuple = (x, y) \n",
    "            if new_tuple not in random_tuples:  # Only add the tuple if it's not already in the set \n",
    "                random_tuples.add(new_tuple) \n",
    "        return random_tuples\n",
    "\n",
    "        \n",
    "class MazeEnvSample5x5(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvSample5x5, self).__init__(maze_file=\"maze2d_5x5.npy\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom5x5(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom5x5, self).__init__(maze_size=(5, 5), enable_render=enable_render, maze_cells=maze_cells, mode='plus', rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvSample10x10(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None, maze_file=None):\n",
    "        super(MazeEnvSample10x10, self).__init__(maze_file=maze_file, enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom10x10(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom10x10, self).__init__(maze_size=(10, 10), enable_render=enable_render, maze_cells=maze_cells, mode='plus', rescue_item_locations=rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvSample3x3(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvSample3x3, self).__init__(maze_file=\"maze2d_3x3.npy\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom3x3(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom3x3, self).__init__(maze_size=(3, 3), enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvSample100x100(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvSample100x100, self).__init__(maze_file=\"maze2d_100x100.npy\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom100x100(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom100x100, self).__init__(maze_size=(100, 100), enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom10x10Plus(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom10x10Plus, self).__init__(maze_size=(10, 10), mode=\"plus\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom20x20Plus(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom20x20Plus, self).__init__(maze_size=(20, 20), mode=\"plus\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)\n",
    "\n",
    "\n",
    "class MazeEnvRandom30x30Plus(MazeEnv):\n",
    "    def __init__(self, enable_render=True, maze_cells=None, rescue_item_locations=None):\n",
    "        super(MazeEnvRandom30x30Plus, self).__init__(maze_size=(30, 30), mode=\"plus\", enable_render=enable_render, maze_cells=maze_cells, rescue_item_locations= rescue_item_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_items_location = [(1, 1), (8, 8), (4, 8), (3, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_maze = np.load(\"hackathon_sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m MazeEnv(maze_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhackathon_sample.npy\u001b[39;49m\u001b[39m'\u001b[39;49m,maze_size\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m,\u001b[39m10\u001b[39;49m), rescue_item_locations\u001b[39m=\u001b[39;49mrescue_items_location, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m, maze_cells\u001b[39m=\u001b[39;49msample_maze)\n",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m, in \u001b[0;36mMazeEnv.__init__\u001b[1;34m(self, maze_file, maze_cells, maze_size, mode, enable_render, rescue_item_locations, has_loops)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_beyond_done \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# Simulation related variables.\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     64\u001b[0m \u001b[39m# Just need to initialize the relevant attributes\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure()\n",
      "Cell \u001b[1;32mIn[1], line 225\u001b[0m, in \u001b[0;36mMazeEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescue_item_locations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_rescue_points()\n\u001b[0;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed()\n\u001b[0;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_locations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_view\u001b[39m.\u001b[39mrescue_item_locations\n",
      "Cell \u001b[1;32mIn[1], line 300\u001b[0m, in \u001b[0;36mMazeEnv.get_rescue_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m random_tuples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()  \u001b[39m# Create an empty set to hold the tuples \u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(random_tuples) \u001b[39m<\u001b[39m \u001b[39m4\u001b[39m:  \u001b[39m# Continue until we have 4 unique tuples \u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     x \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m) \n\u001b[0;32m    301\u001b[0m     y \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m) \n\u001b[0;32m    302\u001b[0m     new_tuple \u001b[39m=\u001b[39m (x, y) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "env = MazeEnv(maze_file='hackathon_sample.npy',maze_size=(10,10), rescue_item_locations=rescue_items_location, mode='human', maze_cells=sample_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.05263158, 0.42105263, 0.31578947,\n",
       "       0.21052632, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import tqdm\n",
    "from stable_baselines3 import PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, buffer_size\u001b[39m=\u001b[39m\u001b[39m1000000\u001b[39m, learning_starts\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, tau\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m, train_freq\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, gradient_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, replay_buffer_class\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, replay_buffer_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, optimize_memory_usage\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, target_update_interval\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, exploration_fraction\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, exploration_initial_eps\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, exploration_final_eps\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, max_grad_norm\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, tensorboard_log\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, policy_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, _init_setup_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "model= DQN(\"MlpPolicy\", env, learning_rate=0.0001, buffer_size=1000000, learning_starts=50000, batch_size=32, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, replay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, target_update_interval=10000, exploration_fraction=0.5, exploration_initial_eps=1.0, exploration_final_eps=0.3, max_grad_norm=3, tensorboard_log=None, policy_kwargs=None, verbose=1, seed=15, device='auto', _init_setup_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55e+04 |\n",
      "|    ep_rew_mean      | -500     |\n",
      "|    exploration_rate | 0.892    |\n",
      "| time/               |          |\n",
      "|    episodes         | 5        |\n",
      "|    fps              | 1826     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 77388    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 6846     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x24248593e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000 , log_interval=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"PPO.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_maze = np.load(\"hackathon_sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(maze_file='hackathon_sample.npy',maze_size=(10,10), rescue_item_locations=rescue_items_location, mode='human', maze_cells=sample_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[505], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_reward, std_reward \u001b[39m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMean reward:\u001b[39m\u001b[39m\"\u001b[39m, mean_reward, \u001b[39m\"\u001b[39m\u001b[39m±\u001b[39m\u001b[39m\"\u001b[39m, std_reward)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(observations, state\u001b[39m=\u001b[39;49mstates, episode_start\u001b[39m=\u001b[39;49mepisode_starts, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[0;32m     89\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m     90\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:343\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[0;32m    344\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    345\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:687\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:721\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \n\u001b[0;32m    717\u001b[0m \u001b[39m:param obs:\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[39m:return: the action distribution.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_features_extractor)\n\u001b[1;32m--> 721\u001b[0m latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor\u001b[39m.\u001b[39;49mforward_actor(features)\n\u001b[0;32m    722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:266\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_actor\u001b[39m(\u001b[39mself\u001b[39m, features: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_net(features))\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=3)\n",
    "print(\"Mean reward:\", mean_reward, \"±\", std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44444444  0.          0.13333333  0.4         0.26666667  0.2\n",
      " -1.          1.          1.          1.          0.          1.\n",
      " -1.          1.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "state, r, done, info = env.step(2)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m state,reward,done,info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action[\u001b[39m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m img \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrender(\u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(img,(\u001b[39m720\u001b[39;49m,\u001b[39m720\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mputText(img,org\u001b[39m=\u001b[39m(\u001b[39m50\u001b[39m,\u001b[39m20\u001b[39m),text\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreward:\u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mdone\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m                     fontFace\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX,fontScale\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,color\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m,\u001b[39m0\u001b[39m))\n\u001b[0;32m     10\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39msss\u001b[39m\u001b[39m'\u001b[39m,img)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import keyboard\n",
    "state =env.reset()\n",
    "for i in range(5000):\n",
    "    action = model.predict(state)\n",
    "    state,reward,done,info = env.step(action[0])\n",
    "    img = env.render('rgb_array')\n",
    "    img = cv2.resize(img,(720,720))\n",
    "    img = cv2.putText(img,org=(50,20),text=f\"reward:{reward:.3f},{done}\",\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=0.5,color=(0,255,0))\n",
    "    cv2.imshow('sss',img)\n",
    "    time.sleep(.01)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
